# Copyright 2022 Canonical Ltd.
# See LICENSE file for licensing details.

options:
  data-dir:
    description: filepath for setting the Kafka dataDir option
    type: string
    default: "/data/kafka"
  log-dir: 
    description: filepath for setting the Kafka dataLogDir option
    type: string
    default: "/logs/kafka"
  offsets-retention-minutes:
    description: the number of minutes offsets will be kept before getting discarded
    type: int
    default: 10080
  log-retention-hours:
    description: the number of hours to keep a log file before deleting it
    type: int
    default: 168
  auto-create-topics:
    description: enables auto creation of topic on the server
    type: boolean
    default: false

      # log.dirs=/var/snap/kafka/common/log
      #
      # # networking
      # clientPort=2181
      # listeners=SASL_PLAINTEXT://:9092
      #
      # # offsets
      # offsets.topic.num.partitions=50
      # offsets.commit.required.acks=-1
      # offsets.retention.minutes=10080
      #
      # # topic
      # auto.leader.rebalance.enable=true
      # # to be changed when necessary
      # delete.topic.enable=true
      # unclean.leader.election.enable=false
      # auto.create.topics.enable=false
      # # helpful
      # group.initial.rebalance.delay.ms=3000
      #
      # # auth
      # sasl.enabled.mechanisms=SCRAM-SHA-512
      # sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512
      # security.inter.broker.protocol=SASL_PLAINTEXT
      # authorizer.class.name=kafka.security.authorizer.AclAuthorizer
      # allow.everyone.if.no.acl.found=false
      # super.users=User:sync
      # listener.name.sasl_plaintext.sasl.enabled.mechanisms=SCRAM-SHA-512
      # # zookeeper.set.acl=true



      ## Backup
      # background.threads=10
      # compression.type=producer
      # leader.imbalance.check.interval.seconds=300
      # leader.imbalance.per.broker.percentage=10
      # log.retention.bytes=-1
      # log.roll.hours=168
      # log.roll.jitter.hours=0
      # log.segment.bytes=1073741824
      # log.segment.delete.delay.ms=60000
      # message.max.bytes=1000012
      # num.io.threads=8
      # num.network.threads=3
      # num.recovery.threads.per.data.dir=1
      # num.replica.fetchers=1
      # offset.metadata.max.bytes=4096
      # offsets.commit.timeout.ms=5000
      # offsets.load.buffer.size=5242880
      # offsets.retention.check.interval.ms=600000
      # offsets.topic.compression.codec=0
      # offsets.topic.segment.bytes=104857600
      # queued.max.requests=500
      # quota.consumer.default=9223372036854775807
      # quota.producer.default=9223372036854775807
      # replica.fetch.min.bytes=1
      # replica.fetch.wait.max.ms=500
      # replica.high.watermark.checkpoint.interval.ms=5000
      # replica.lag.time.max.ms=10000
      # replica.socket.receive.buffer.bytes=65536
      # replica.socket.timeout.ms=30000
      # request.timeout.ms=30000
      # socket.receive.buffer.bytes=102400
      # socket.request.max.bytes=104857600
      # socket.send.buffer.bytes=102400
      # zookeeper.session.timeout.ms=6000
      # connections.max.idle.ms=600000
      # controlled.shutdown.enable=true
      # controlled.shutdown.max.retries=3
      # controlled.shutdown.retry.backoff.ms=5000
      # controller.socket.timeout.ms=30000
      # fetch.purgatory.purge.interval.requests=1000
      # group.max.session.timeout.ms=300000
      # group.min.session.timeout.ms=600
      # producer.purgatory.purge.interval.requests=1000
      # replica.fetch.backoff.ms=1000
      # replica.fetch.max.bytes=1048576
      # replica.fetch.response.max.bytes=10485760
      # reserved.broker.max.id=1000
      # num.partitions=1
      # group.initial.rebalance.delay.ms=0
      # zookeeper.connection.timeout.ms=18000
